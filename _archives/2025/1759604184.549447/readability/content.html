<meta charset="UTF-8">
<div id="readability-page-1" class="page"><div data-testid="inline-content" id="comp-mb186tco8" data-mesh-id="comp-mb186tcn5inlineContent"><p>This workshop aims to explore the integration of multimodal inputs, particularly visual and textural data, into Automatic Speech Recognition (ASR) systems to enhance their performance and applicability. By bringing together researchers and practitioners, we will discuss recent advancements, challenges, and future directions in Multimodal Natural Language Processing (MNLP) with a focus on ASR.</p>

<p><span>​</span></p>

<p>Traditional ASR systems primarily rely on acoustic signals to transcribe speech. However, human communication is inherently multimodal, often involving visual cues such as lip movements, gestures, and contextual imagery. Integrating these additional modalities can significantly improve the accuracy and robustness of ASR systems, especially in noisy environments or when dealing with ambiguous linguistic content. This workshop seeks to:</p>

<ul>
<li>
<p><span>Highlight Recent Research:</span> Present cutting-edge studies that incorporate visual/textural information into ASR, demonstrating improved performance and new capabilities.</p>
</li>
<li>
<p><span>Discuss Technical Challenges:</span> Identify and address the technical hurdles in fusing multimodal data, such as synchronization, data representation, and model integration.</p>
</li>
<li>
<p><span>Explore Applications:</span> Examine real-world applications where multimodal ASR offers significant advantages, including assistive technologies, human-computer interaction, and multimedia content analysis.</p>
</li>
<li>
<p><span>Foster Collaboration:</span> Create a platform for interdisciplinary collaboration among researchers in speech processing, computer vision, and natural language processing.</p>
</li>
</ul>

<p><span><span>​</span>​</span></p>

<p><span><span>​</span>​</span></p>

<h2><span>Workshop Highlights</span></h2>

<p>The workshop will be structured as a full-day event, comprising:</p>

<ul>
<li>
<p><span>Keynote Speeches:</span> Invited talks from leading experts in multimodal ASR and MNLP.</p>
</li>
<li>
<p><span>Paper Presentations:</span> Oral presentations of peer-reviewed research papers.</p>
</li>
<li>
<p><span>Poster Sessions:</span> Interactive sessions for researchers to showcase their work and engage in detailed discussions.</p>
</li>
<li>
<p><span>Panel Discussion: </span>A moderated discussion on the future directions of multimodal ASR, involving academia and industry experts.</p>
</li>
<li>
<p><span>Hands-on Tutorials:</span> Practical sessions demonstrating tools and techniques for integrating multimodal data into ASR systems.</p>
</li>
</ul>

<p><span>​</span></p>

<p><span>​</span></p>

<h2><span>Target Audience</span></h2>

<ul>
<li>
<p><span>Researchers and Academics:</span> Individuals working in ASR, computer vision, and MNLP.</p>
</li>
<li>
<p><span>Industry Professionals:</span> Practitioners developing speech recognition applications and interested in multimodal integration.</p>
</li>
<li>
<p><span>Graduate Students:</span> Students seeking to deepen their understanding of multimodal approaches in speech recognition.</p>
</li>
</ul>

<p><span>​</span></p>

<p><span>​</span></p>

<h2><span><span>​</span>​</span><span>​</span><span>Call for Papers</span>​</h2>

<p>We invite submissions on topics including, but not limited to:</p>

<ul>
<li>
<p><span>Multimodal Data Fusion:</span> Techniques for combining audio and visual/textural data in ASR systems.</p>
</li>
<li>
<p><span>Model Architectures: </span>Innovative neural architectures for multimodal ASR.</p>
</li>
<li>
<p><span>Dataset Development:</span> Creation and annotation of datasets containing synchronized audio and visual data.</p>
</li>
<li>
<p><span>Application Case Studies:</span> Real-world implementations of multimodal ASR systems.</p>
</li>
<li>
<p><span>Evaluation Metrics:</span> Methods for assessing the performance of multimodal ASR systems.</p>
</li>
</ul>

<p><span><span><span>​</span>​</span></span></p>

<p><span><span>​</span></span></p>

<p><span><span><span>Submission Guidelines</span></span></span></p>

<ul>
<li>
<p><span>Paper Length:</span> Long papers (up to 12 pages) and short papers (up to 6 pages), including references.</p>
</li>
<li>
<p><span>Format:</span> Submissions should follow the conference's specified template.</p>
</li>
<li>
<p><span>Review Process:</span> All submissions will undergo double-blind peer review.</p>
</li>
<li>
<p><span>Submission Link:</span> [placeholder]</p>
</li>
</ul>

<p><span>​</span></p>

<p><span>​</span></p>

<p><span><span><span>Important Dates</span></span></span></p>

<ul>
<li>
<p>Paper Submission Deadline: [Insert Date]​</p>
</li>
<li>
<p>Notification of Acceptance: [Insert Date]​</p>
</li>
<li>
<p>Camera-Ready Papers Due: [Insert Date]​</p>
</li>
<li>
<p>Workshop Date: [Insert Date]​</p>
</li>
</ul>

<p><span>​</span></p>

<p><span>​</span></p>

<p><span><span><span>Workshop Organizers </span></span></span></p>

<ul>
<li>
<p>Sam Rahbar, DialPad, <a href="mailto:info@srahbar.com" data-auto-recognition="true">info@srahbar.com</a></p>
</li>
<li>
<p>[Name], [Affiliation], [Contact Information]​</p>
</li>
<li>
<p>[Name], [Affiliation], [Contact Information]​</p>
</li>
</ul>

<p><span>​</span></p>

<p><span>​</span></p>

<p><span><span>Potential Invited Speakers</span></span></p>

<ul>
<li>
<p>Dr. [Name], [Affiliation], expertise in multimodal ASR.​</p>
</li>
<li>
<p>Dr. [Name], [Affiliation], known for work in MNLP.</p>
</li>
</ul>

<p><span>​</span></p>

<p><span>​</span></p>

<p><span><span><span>Workshop Schedule</span></span></span></p>

<ul>
<li>
<p>9:00 AM - 9:15 AM: Opening Remarks​</p>
</li>
<li>
<p>9:15 AM - 10:00 AM: Keynote Speech 1​</p>
</li>
<li>
<p>10:00 AM - 11:00 AM: Paper Session 1​</p>
</li>
<li>
<p>11:00 AM - 11:15 AM: Coffee Break​</p>
</li>
<li>
<p>11:15 AM - 12:15 PM: Paper Session 2​</p>
</li>
<li>
<p>12:15 PM - 1:15 PM: Lunch Break​</p>
</li>
<li>
<p>1:15 PM - 2:00 PM: Keynote Speech 2​</p>
</li>
<li>
<p>2:00 PM - 3:00 PM: Poster Session​</p>
</li>
<li>
<p>3:00 PM - 3:15 PM: Coffee Break​</p>
</li>
<li>
<p>3:15 PM - 4:15 PM: Panel Discussion​</p>
</li>
<li>
<p>4:15 PM - 5:15 PM: Hands-on Tutorial​</p>
</li>
<li>
<p>5:15 PM - 5:30 PM: Closing Remarks​</p>
</li>
</ul>

<p><span><span>​</span>​​</span></p>

<p><span>​</span></p></div></div>